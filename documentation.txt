Tokenization
Sadece whitespace karakterlere göre ayırmak iyi bir çözüm değil. Url, hashtag, email adresleri özel. 
Çok kelimeli yapılar var alt kategori gibi New York gibi bunlar ayrılmadan bütün değerlendirilmeli.
Büşra'nın ı Büşra ve ' nın olarak ayıramazsın bir bütün. kesme işaretine dikkat. 
Fiyatı (45.53 TL) tarihi (01/02/2020) ayıramazsın 
virgül kullanışlı o varsa ayırabilirsin direkt
Sunumda bunlar anlatılıyor ama tokenizer için sadece boşlukla ayırma ve öncesinde bazı özel karakterleri silme yapılıyormuş her yerde öyle

Stopwords elimination ın ilk işlemi için:
Kelime listesini iki farklı kaynaktan topladık https://www.kaggle.com/tbrknt/detection-of-cyberbullying-in-turkish ve https://github.com/tkorkunckaya/Turkish-Stopwords/blob/master/stopwords/mongo/stop_words_turkish.txt
Dynamic approach of stopword elimination için:
One way is to look at word frequencies as a whole.
Calculate the frequency of all words in the combined texts. Sort them in falling order and remove the top 20% or so.
You may also wish to remove the bottom 5%. These are not stop-words, but for a lot of machine learning, they are inconsequential. Maybe even misspellings.
